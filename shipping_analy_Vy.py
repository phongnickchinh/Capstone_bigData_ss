from spark_utils import create_spark_session_with_mysql
from config import DATABASE_CONFIG, ANALYSIS_TABLES, FILE_PATHS
from pyspark.sql.functions import lit, avg, col, count
import datetime


# T·∫°o Spark session v·ªõi MySQL connector
spark = create_spark_session_with_mysql()

# K·∫øt n·ªëi MySQL t·ª´ config
mysql_url = DATABASE_CONFIG["mysql_url"]
mysql_props = DATABASE_CONFIG["mysql_props"]
hdfs_path = FILE_PATHS["hdfs_raw"]

# ƒê·ªçc d·ªØ li·ªáu ƒë√£ preprocessed
# ∆Øu ti√™n ƒë·ªçc t·ª´ HDFS n·∫øu c√≥, ng∆∞·ª£c l·∫°i t·ª´ local
df = None
# ƒë·ªçc t·ª´ th∆∞ m·ª•c
if hdfs_path:
    try:
        df = spark.read.csv(hdfs_path, header=True, inferSchema=True)
        print(f"ƒê√£ ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS: {hdfs_path}")
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc t·ª´ HDFS: {e}. S·∫Ω th·ª≠ ƒë·ªçc t·ª´ local.")
else:
    print("Kh√¥ng c√≥ ƒë∆∞·ªùng d·∫´n HDFS, s·∫Ω ƒë·ªçc t·ª´ local.")
    df = spark.read.csv("file:///home/phamp/Train_cleaned.csv", header=True, inferSchema=True)
df.printSchema()
df.show(5)


from pyspark.sql.functions import avg, col, lit

# ============================================================================
# 1. T·ªîNG QUAN HI·ªÜU SU·∫§T GIAO H√ÄNG - OVERALL PERFORMANCE
# ============================================================================
print("üìä 1. PH√ÇN T√çCH T·ªîNG QUAN HI·ªÜU SU·∫§T GIAO H√ÄNG")
avg_df = df.select(avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"))
result_df = avg_df.withColumn("late_rate", lit(1) - col("on_time_rate"))
#th√™m t·ªïng ƒë∆°n
result_df = result_df.withColumn("total_orders", lit(df.count()))

result_df.show()

# Ghi v√†o MySQL
result_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["overall_performance"],
                    mode="overwrite", properties=mysql_props)

# ============================================================================
# 2. PH√ÇN T√çCH THEO KHU V·ª∞C KHO - WAREHOUSE BLOCK ANALYSIS
# ============================================================================
print("üè¢ 2. PH√ÇN T√çCH THEO KHU V·ª∞C KHO")
warehouse_df = df.groupBy("Warehouse_block") \
	.agg(
        avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
        count("*").alias("count")
    ) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("late_rate", ascending=False)
warehouse_df.show()

# Ghi v√†o MySQL
warehouse_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["warehouse_analysis"], 
                        mode="overwrite", properties=mysql_props)

# ============================================================================
# 3. PH√ÇN T√çCH THEO PH∆Ø∆†NG TH·ª®C V·∫¨N CHUY·ªÇN - SHIPPING MODE ANALYSIS
# ============================================================================
print("üöö 3. PH√ÇN T√çCH THEO PH∆Ø∆†NG TH·ª®C V·∫¨N CHUY·ªÇN")
shipping_mode_df = df.groupBy("Mode_of_Shipment") \
	.agg(
        avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
        count("*").alias("count")
    ) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("late_rate", ascending=False)
shipping_mode_df.show()

# Ghi v√†o MySQL
shipping_mode_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["shipping_mode_analysis"], 
                            mode="overwrite", properties=mysql_props)

# ============================================================================
# 4. PH√ÇN T√çCH THEO ƒê·ªò QUAN TR·ªåNG S·∫¢N PH·∫®M - PRODUCT IMPORTANCE ANALYSIS
# ============================================================================
print("‚≠ê 4. PH√ÇN T√çCH THEO ƒê·ªò QUAN TR·ªåNG S·∫¢N PH·∫®M")
product_importance_df = df.groupBy("Product_importance") \
	.agg(
        avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
        count("*").alias("count")
    ) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("late_rate", ascending=False)
product_importance_df.show()

# Ghi v√†o MySQL
product_importance_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["product_importance_analysis"], 
                                mode="overwrite", properties=mysql_props)

from pyspark.sql.functions import when

# ============================================================================
# 5. PH√ÇN T√çCH THEO M·ª®C ƒê·ªò GI·∫¢M GI√Å - DISCOUNT LEVEL ANALYSIS
# ============================================================================
print("üí∞ 5. PH√ÇN T√çCH THEO M·ª®C ƒê·ªò GI·∫¢M GI√Å")
# T·∫°o categories: Low (<=10%), Medium (10-30%), High (>30%)
df = df.withColumn("Discount_level",
    when(col("Discount_offered") <= 7, "Low")
    .when((col("Discount_offered") > 7) & (col("Discount_offered") <= 20.5), "Medium")
    .otherwise("High")
)
discount_df = df.groupBy("Discount_level") \
	.agg(
		avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
		count("*").alias("count")
	) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("late_rate", ascending=False)
discount_df.show()

# Ghi v√†o MySQL
discount_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["discount_level_analysis"], 
                    mode="overwrite", properties=mysql_props)

# ============================================================================
# 6. PH√ÇN T√çCH THEO DANH M·ª§C TR·ªåNG L∆Ø·ª¢NG - WEIGHT CATEGORY ANALYSIS
# ============================================================================
print("‚öñÔ∏è 6. PH√ÇN T√çCH THEO DANH M·ª§C TR·ªåNG L∆Ø·ª¢NG")
# T·∫°o categories: Light (<=2000g), Medium (2000-4000g), Heavy (>4000g)

df = df.withColumn("Weight_category",
    when(col("Weight_in_gms") <= 3000, "Light")
    .when((col("Weight_in_gms") > 3000) & (col("Weight_in_gms") <= 5500), "Medium")
    .otherwise("Heavy")
)
#in s·ªë l∆∞·ª£ng m·ªói ph√¢n lo·∫°i
weight_category_counts = df.groupBy("Weight_category").agg(count("*").alias("count"))
weight_category_counts.show()


weight_df = df.groupBy("Weight_category") \
	.agg(avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
		count("*").alias("count")
	) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("late_rate", ascending=False)
weight_df.show()

# Ghi v√†o MySQL
weight_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["weight_category_analysis"], 
                    mode="overwrite", properties=mysql_props)

# ============================================================================
# 7. PH√ÇN T√çCH K·∫æT H·ª¢P - SHIPPING MODE & WEIGHT CATEGORY ANALYSIS
# ============================================================================
print("üöõ 7. PH√ÇN T√çCH K·∫æT H·ª¢P: PH∆Ø∆†NG TH·ª®C V·∫¨N CHUY·ªÇN & TR·ªåNG L∆Ø·ª¢NG")
# Ph√¢n t√≠ch cross-dimensional: Ph∆∞∆°ng th·ª©c v·∫≠n chuy·ªÉn √ó Tr·ªçng l∆∞·ª£ng
combined_df = df.groupBy("Mode_of_Shipment", "Weight_category") \
	.agg(avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
		count("*").alias("count")
	) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("late_rate", ascending=False)
combined_df.show()

# Ghi v√†o MySQL
combined_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["combined_mode_weight"], 
                       mode="overwrite", properties=mysql_props)

# ============================================================================
# 8. PH√ÇN T√çCH THEO ƒê√ÅNH GI√Å KH√ÅCH H√ÄNG - CUSTOMER RATING ANALYSIS
# ============================================================================
print("‚≠ê 8. PH√ÇN T√çCH THEO ƒê√ÅNH GI√Å KH√ÅCH H√ÄNG")
# Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa ƒë√°nh gi√° kh√°ch h√†ng v√† hi·ªáu su·∫•t giao h√†ng
rating_df = df.groupBy("Customer_rating") \
	.agg(avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
		count("*").alias("count")
	) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("Customer_rating")
rating_df.show()

# Ghi v√†o MySQL
rating_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["customer_rating_analysis"], 
                    mode="overwrite", properties=mysql_props)

# ============================================================================
# 9. PH√ÇN T√çCH THEO S·ªê CU·ªòC G·ªåI CHƒÇM S√ìC - CUSTOMER CARE CALLS ANALYSIS
# ============================================================================
print("üìû 9. PH√ÇN T√çCH THEO S·ªê CU·ªòC G·ªåI CHƒÇM S√ìC KH√ÅCH H√ÄNG")
# Ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa s·ªë cu·ªôc g·ªçi chƒÉm s√≥c kh√°ch h√†ng ƒë·∫øn hi·ªáu su·∫•t giao h√†ng
care_calls_df = df.groupBy("Customer_care_calls") \
	.agg(avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
		count("*").alias("count")
	) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("Customer_care_calls")
care_calls_df.show()

# Ghi v√†o MySQL
care_calls_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["customer_care_analysis"], 
                        mode="overwrite", properties=mysql_props)

# ============================================================================
# 10. PH√ÇN T√çCH THEO L·ªäCH S·ª¨ MUA H√ÄNG - PRIOR PURCHASES ANALYSIS
# ============================================================================
print("üõí 10. PH√ÇN T√çCH THEO L·ªäCH S·ª¨ MUA H√ÄNG")
# Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa l·ªãch s·ª≠ mua h√†ng v√† hi·ªáu su·∫•t giao h√†ng
purchases_df = df.groupBy("Prior_purchases") \
	.agg(avg(col("`Reached.on.Time_Y.N`")).alias("on_time_rate"),
		count("*").alias("count")
	) \
	.withColumn("late_rate", 1 - col("on_time_rate")) \
	.orderBy("Prior_purchases")
purchases_df.show()

# Ghi v√†o MySQL
purchases_df.write.jdbc(url=mysql_url, table=ANALYSIS_TABLES["prior_purchases_analysis"], 
                        mode="overwrite", properties=mysql_props)

print("‚úÖ ƒê√£ ho√†n th√†nh t·∫•t c·∫£ 10 ph√¢n t√≠ch v√† ghi d·ªØ li·ªáu v√†o MySQL!")
spark.stop()
