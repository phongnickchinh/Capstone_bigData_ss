#!/usr/bin/env python3
"""
Script cháº¡y trá»±c tiáº¿p trong WSL - khÃ´ng cáº§n gá»i wsl command
"""

import subprocess
import sys
import os
from datetime import datetime

class WSLSparkRunner:
    def __init__(self):
        self.project_path = "/mnt/p/coddd/Capstone_group4"
        
    def run_command(self, command, description="", show_output=True):
        """Cháº¡y co        print("\nğŸ“‹ MENU:")
        print("1. ğŸ” Check installations")
        print("2. â˜• Setup Java 11") 
        print("3. âš™ï¸  Setup Hadoop & Spark")
        print("4. ğŸ”§ Check MySQL JAR")
        print("5. ğŸš€ Start Hadoop only")
        print("6. âš¡ Run preprocessing only")
        print("7. ğŸ“Š Run analysis only") 
        print("8. ğŸ”— Run correlation analysis")
        print("9. ğŸ¯ Run FULL PIPELINE")
        print("0. âŒ Exit") tiáº¿p trong WSL"""
        if description:
            print(f"\nğŸ”„ {description}")
        
        try:
            # Change to project directory first
            full_command = f"cd '{self.project_path}' && {command}"
            
            result = subprocess.run(full_command, 
                                  shell=True, 
                                  capture_output=not show_output,
                                  text=True,
                                  executable='/bin/bash')
            
            if result.returncode == 0:
                if not show_output and result.stdout:
                    print(result.stdout.strip())
                return True, result.stdout if not show_output else ""
            else:
                if not show_output:
                    print(f"âŒ Error: {result.stderr}")
                return False, result.stderr if not show_output else ""
                
        except Exception as e:
            print(f"âŒ Exception: {e}")
            return False, str(e)
    
    def run_script(self, script_content, description=""):
        """Cháº¡y multi-line script trá»±c tiáº¿p"""
        if description:
            print(f"\nğŸ”„ {description}")
        
        try:
            # Create temp script file
            temp_script = f"/tmp/spark_script_{os.getpid()}.sh"
            
            # Write script to file
            with open(temp_script, 'w') as f:
                f.write("#!/bin/bash\n")
                f.write(f"cd '{self.project_path}'\n")
                f.write(script_content)
            
            # Make executable
            os.chmod(temp_script, 0o755)
            
            # Run script
            result = subprocess.run(['/bin/bash', temp_script], 
                                  capture_output=False, 
                                  text=True)
            
            # Cleanup
            try:
                os.remove(temp_script)
            except:
                pass
            
            return result.returncode == 0, ""
                
        except Exception as e:
            print(f"âŒ Exception: {e}")
            return False, str(e)
    
    def check_installations(self):
        """Kiá»ƒm tra cÃ¡c cÃ i Ä‘áº·t cáº§n thiáº¿t"""
        print("\nğŸ” CHECKING INSTALLATIONS")
        print("-" * 40)
        
        # Check Java
        success, output = self.run_command("java -version 2>&1", show_output=False)
        if success and "11." in output:
            print("âœ… Java 11: Found")
        else:
            print("âŒ Java 11: Not found")
        
        # Check Hadoop
        success, _ = self.run_command("ls -la /home/phamp/hadoop/bin/hadoop", show_output=False)
        if success:
            print("âœ… Hadoop: Found")
        else:
            print("âŒ Hadoop: Not found")
        
        # Check Spark
        success, _ = self.run_command("ls -la /home/phamp/spark/bin/spark-submit", show_output=False)
        if success:
            print("âœ… Spark: Found")
        else:
            print("âŒ Spark: Not found")
        
        # Check PySpark
        success, _ = self.run_command("python3 -c 'import pyspark; print(pyspark.__version__)'", show_output=False)
        if success:
            print("âœ… PySpark: Found")
        else:
            print("âŒ PySpark: Not found")
        
        # Check MySQL JAR
        self.check_mysql_jar()
    
    def check_mysql_jar(self):
        """Kiá»ƒm tra MySQL JAR file"""
        print("\nğŸ” CHECKING MYSQL JAR")
        print("-" * 30)

        jar_path = "/home/phamp/jars/mysql-connector-j-8.3.0.jar"
        success, _ = self.run_command(f"ls -la {jar_path}", show_output=False)
        if success:
            print("âœ… MySQL JAR found")
            return True
        else:
            print("âŒ MySQL JAR not found")
            return False
    
    def check_java(self):
        """Kiá»ƒm tra Java"""
        print("\nâ˜• KIá»‚M TRA JAVA")
        print("-" * 30)
        
        success, output = self.run_command("java -version 2>&1", show_output=False)
        if success and "11." in output:
            print(f"âœ… Java 11 found")
            return True
        elif success:
            print(f"âš ï¸  Java found but not version 11")
            return False
        else:
            print("âŒ Java not found")
            return False
    
    def setup_java(self):
        """Thiáº¿t láº­p Java 11"""
        print("\nğŸ“¦ SETUP JAVA 11")
        print("-" * 30)
        
        commands = [
            ("sudo apt update", "Updating package list"),
            ("sudo apt install -y openjdk-11-jdk", "Installing Java 11"),
            ("echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc", "Setting JAVA_HOME"),
            ("echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc", "Adding Java to PATH")
        ]
        
        for cmd, desc in commands:
            success, _ = self.run_command(cmd, desc, show_output=False)
            if success:
                print(f"âœ… {desc}")
            else:
                print(f"âŒ Failed: {desc}")
                return False
        
        return True
    
    def setup_hadoop_spark(self):
        """CÃ i Ä‘áº·t Hadoop vÃ  Spark"""
        print("\nâš™ï¸  SETUP HADOOP & SPARK")
        print("-" * 30)
        
        setup_script = '''
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Táº¡o thÆ° má»¥c
sudo mkdir -p /opt
cd /tmp

# CÃ i Hadoop náº¿u chÆ°a cÃ³
if [ ! -d "/opt/hadoop" ]; then
    echo "Installing Hadoop..."
    wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
    sudo tar -xzf hadoop-3.3.6.tar.gz -C /opt
    sudo mv /opt/hadoop-3.3.6 /opt/hadoop
    sudo chown -R $USER:$USER /opt/hadoop
fi

# CÃ i Spark náº¿u chÆ°a cÃ³
if [ ! -d "/opt/spark" ]; then
    echo "Installing Spark..."
    wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
    sudo tar -xzf spark-3.4.1-bin-hadoop3.tgz -C /opt
    sudo mv /opt/spark-3.4.1-bin-hadoop3 /opt/spark
    sudo chown -R $USER:$USER /opt/spark
fi

# Setup environment
grep -q "HADOOP_HOME" ~/.bashrc || echo "export HADOOP_HOME=/opt/hadoop" >> ~/.bashrc
grep -q "SPARK_HOME" ~/.bashrc || echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
grep -q "HADOOP_HOME/bin" ~/.bashrc || echo "export PATH=\\$PATH:/opt/hadoop/bin:/opt/hadoop/sbin" >> ~/.bashrc
grep -q "SPARK_HOME/bin" ~/.bashrc || echo "export PATH=\\$PATH:/opt/spark/bin:/opt/spark/sbin" >> ~/.bashrc
grep -q "PYTHONPATH" ~/.bashrc || echo "export PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-*.zip:\\$PYTHONPATH" >> ~/.bashrc

# Cáº¥u hÃ¬nh Hadoop
sudo mkdir -p /opt/hadoop/etc/hadoop
cat > /tmp/core-site.xml << 'EOF'
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
EOF
sudo mv /tmp/core-site.xml /opt/hadoop/etc/hadoop/

echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> /opt/hadoop/etc/hadoop/hadoop-env.sh

echo "Setup completed!"
'''
        
        success, _ = self.run_script(setup_script, "Setting up Hadoop & Spark")
        return success
    
    def start_hadoop(self):
        """Khá»Ÿi Ä‘á»™ng Hadoop"""
        print("\nğŸš€ STARTING HADOOP")
        print("-" * 30)
        
        start_script = '''
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/home/phamp/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Source bashrc to get environment
source ~/.bashrc

# Check if Hadoop is already running
if jps | grep -q "NameNode\|DataNode"; then
    echo "âœ… Hadoop services are already running:"
    jps | grep -E "(NameNode|DataNode|SecondaryNameNode|ResourceManager|NodeManager)"
else
    echo "Starting Hadoop services..."
    
    # Format namenode láº§n Ä‘áº§u náº¿u cáº§n
    if [ ! -d "/tmp/hadoop-$USER" ]; then
        echo "Formatting namenode..."
        echo "Y" | $HADOOP_HOME/bin/hdfs namenode -format
    fi

    # Start DFS services
    $HADOOP_HOME/sbin/start-dfs.sh
    
    # Start YARN services  
    $HADOOP_HOME/sbin/start-yarn.sh
    
    # Wait for services to start
    echo "Waiting for services to start..."
    sleep 10
fi

# Ensure HDFS directories exist
echo "Setting up HDFS directories..."
$HADOOP_HOME/bin/hdfs dfs -mkdir -p /input 2>/dev/null || echo "Input directory already exists"
$HADOOP_HOME/bin/hdfs dfs -mkdir -p /output 2>/dev/null || echo "Output directory already exists"

# Upload data file
echo "Uploading data file..."
if $HADOOP_HOME/bin/hdfs dfs -test -e /input/Train.csv; then
    echo "âœ… Train.csv already exists in HDFS"
else
    if [ -f "Train.csv" ]; then
        $HADOOP_HOME/bin/hdfs dfs -put Train.csv /input/
        echo "âœ… Train.csv uploaded to HDFS"
    else
        echo "âŒ Train.csv not found in current directory"
    fi
fi

echo "Hadoop setup completed!"
echo "Current running processes:"
jps
'''
        
        return self.run_script(start_script, "Starting Hadoop services")
    
    def run_spark_job(self, script_name):
        """Cháº¡y Spark job"""
        print(f"\nâš¡ RUNNING {script_name}")
        print("-" * 30)
        
        # Check MySQL JAR first
        if not self.check_mysql_jar():
            print("âŒ MySQL JAR not found. Cannot continue.")
            return False
        
        spark_script = f'''
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/home/phamp/hadoop
export SPARK_HOME=/home/phamp/spark
export PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*.zip:$PYTHONPATH

# Source bashrc
source ~/.bashrc

# Check if MySQL JAR exists
JAR_PATH="/home/phamp/jars/mysql-connector-j-8.3.0.jar"
if [ ! -f "$JAR_PATH" ]; then
    echo "âŒ MySQL JAR not found at $JAR_PATH"
    exit 1
fi

echo "Running {script_name} with MySQL connector..."
echo "Using JAR: $JAR_PATH"
echo "SPARK_HOME: $SPARK_HOME"
echo "HADOOP_HOME: $HADOOP_HOME"

# Use spark-submit to ensure JAR is properly loaded with reduced logging
$SPARK_HOME/bin/spark-submit \\
    --jars $JAR_PATH \\
    --driver-class-path $JAR_PATH \\
    --conf spark.executor.extraClassPath=$JAR_PATH \\
    --conf spark.driver.extraClassPath=$JAR_PATH \\
    --conf spark.sql.warehouse.dir=/tmp/spark-warehouse \\
    --conf spark.sql.adaptive.enabled=true \\
    --conf spark.sql.adaptive.coalescePartitions.enabled=true \\
    --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties" \\
    --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties" \\
    {script_name} 2>/dev/null
'''
        
        return self.run_script(spark_script, f"Running {script_name}")
    
    def run_correlation_analysis(self):
        """Cháº¡y correlation analysis vá»›i ML preprocessing"""
        print("\nğŸ”— CORRELATION ANALYSIS")
        print("=" * 50)
        
        # Kiá»ƒm tra xem cÃ³ preprocessed data khÃ´ng
        check_cmd = "ls -la preprocessed_data/*.csv 2>/dev/null | wc -l"
        success, output = self.run_command(check_cmd, "", show_output=False)
        
        if success and output.strip() == "0":
            print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y preprocessed data. Cháº¡y preprocessing trÆ°á»›c...")
            if not self.run_spark_job("spark_preprocessing.py"):
                print("âŒ Preprocessing failed")
                return False
        
        # Cháº¡y correlation analysis
        return self.run_spark_job("machine_learning/run_correlation_analysis.py")

    def full_pipeline(self):
        """Cháº¡y full pipeline"""
        print("\nğŸ¯ FULL PIPELINE")
        print("=" * 50)
        
        # Start Hadoop
        if not self.start_hadoop():
            print("âŒ Failed to start Hadoop")
            return False
        
        # Run preprocessing
        if not self.run_spark_job("spark_preprocessing.py"):
            print("âŒ Preprocessing failed")
            return False
        
        # Run analysis
        if not self.run_spark_job("analysis/shipping_analy.py"):
            print("âŒ Analysis failed")
            return False
        
        print("\nğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
        return True

def main():
    """Menu chÃ­nh"""

    runner = WSLSparkRunner()
    
    print("ğŸ”¥ WSL SPARK RUNNER - NATIVE WSL VERSION")
    print("=" * 50)
    
    while True:
        print("\nğŸ“‹ MENU:")
        print("1. ğŸ” Check installations")
        print("2. â˜• Setup Java 11") 
        print("3. âš™ï¸  Setup Hadoop & Spark")
        print("4. ï¿½ Check MySQL JAR")
        print("5. ï¿½ğŸš€ Start Hadoop only")
        print("6. âš¡ Run preprocessing only")
        print("7. ğŸ“Š Run analysis only") 
        print("8. ğŸ¯ Run FULL PIPELINE")
        print("0. âŒ Exit")
        
        choice = input("\nChá»n (0-9): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ Goodbye!")
            break
        elif choice == "1":
            runner.check_installations()
        elif choice == "2":
            runner.setup_java()
        elif choice == "3":
            runner.setup_hadoop_spark()
        elif choice == "4":
            runner.check_mysql_jar()
        elif choice == "5":
            runner.start_hadoop()
        elif choice == "6":
            runner.run_spark_job("spark_preprocessing.py")
        elif choice == "7":
            runner.run_spark_job("analysis/shipping_analy.py")
        elif choice == "8":
            runner.run_correlation_analysis()
        elif choice == "9":
            runner.full_pipeline()
        else:
            print("âŒ Invalid choice")

if __name__ == "__main__":
    main()
