from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from analysis.config import SPARK_CONFIG, FILE_PATHS, COLUMN_MAPPINGS

def create_spark_session():
    """T·∫°o Spark session cho ML preprocessing"""
    builder = SparkSession.builder.appName("ML_Data_Preprocessing")
    
    # Th√™m c√°c config t·ª´ file config
    for key, value in SPARK_CONFIG["preprocessing"]["configs"].items():
        builder = builder.config(key, value)
    
    return builder.getOrCreate()

def load_cleaned_data(spark, file_path):
    """ƒê·ªçc d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c cleaned t·ª´ preprocessing pipeline"""
    try:
        # ƒê·ªçc t·ª´ preprocessed data thay v√¨ raw data
        df = spark.read.csv(file_path, header=True, inferSchema=True)
        print(f"ƒê√£ ƒë·ªçc {df.count()} records t·ª´ {file_path}")
        return df
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file: {e}")
        return None

def prepare_ml_features(df):
    """Chu·∫©n b·ªã features cho Machine Learning"""
    
    # 1. StringIndexer cho categorical features
    categorical_cols = ["warehouse_block", "mode_of_shipment", "product_importance"]
    if "gender" in df.columns:  # N·∫øu c√≥ c·ªôt gender
        categorical_cols.append("gender")
    
    indexers = []
    for col_name in categorical_cols:
        if col_name in df.columns:
            indexer = StringIndexer(
                inputCol=col_name, 
                outputCol=f"{col_name}_index",
                handleInvalid="keep"  # Handle unknown categories
            )
            indexers.append(indexer)
    
    # StringIndexer cho target variable
    if "reached_on_time" in df.columns:
        label_indexer = StringIndexer(
            inputCol="reached_on_time", 
            outputCol="label",
            handleInvalid="keep"
        )
        indexers.append(label_indexer)
    
    # 2. OneHotEncoder cho categorical features
    encoder_input_cols = [f"{col}_index" for col in categorical_cols if col in df.columns]
    encoder_output_cols = [f"{col}_ohe" for col in categorical_cols if col in df.columns]
    
    encoder = OneHotEncoder(
        inputCols=encoder_input_cols,
        outputCols=encoder_output_cols,
        handleInvalid="keep"
    )
    
    # 3. Numerical features ƒë·ªÉ scaling
    numeric_cols = ["customer_care_calls", "customer_rating", "cost_of_product", 
                   "prior_purchases", "discount_offered", "weight_in_gms"]
    
    # L·ªçc ch·ªâ nh·ªØng c·ªôt c√≥ trong DataFrame
    available_numeric_cols = [col for col in numeric_cols if col in df.columns]
    
    # VectorAssembler cho numeric features
    numeric_assembler = VectorAssembler(
        inputCols=available_numeric_cols, 
        outputCol="numeric_features",
        handleInvalid="keep"
    )
    
    # 4. StandardScaler cho numeric features
    scaler = StandardScaler(
        inputCol="numeric_features", 
        outputCol="scaled_features", 
        withMean=True, 
        withStd=True
    )
    
    # 5. Final VectorAssembler ƒë·ªÉ g·ªôp t·∫•t c·∫£ features
    final_feature_cols = encoder_output_cols + ["scaled_features"]
    
    final_assembler = VectorAssembler(
        inputCols=final_feature_cols,
        outputCol="features",
        handleInvalid="keep"
    )
    
    # 6. T·∫°o ML Pipeline
    stages = indexers + [encoder, numeric_assembler, scaler, final_assembler]
    
    pipeline = Pipeline(stages=stages)
    
    return pipeline

def create_ml_dataset(df, pipeline):
    """T·∫°o dataset cho machine learning"""
    # Fit v√† transform pipeline
    pipeline_model = pipeline.fit(df)
    transformed_df = pipeline_model.transform(df)
    
    # Select ch·ªâ features v√† label c·∫ßn thi·∫øt cho ML
    if "label" in transformed_df.columns:
        ml_df = transformed_df.select("features", "label")
    else:
        # N·∫øu kh√¥ng c√≥ label (unsupervised learning)
        ml_df = transformed_df.select("features")
    
    return ml_df, pipeline_model

def save_ml_dataset(ml_df, output_path):
    """L∆∞u ML dataset"""
    try:
        # L∆∞u d∆∞·ªõi d·∫°ng parquet cho ML
        ml_df.write.mode("overwrite").parquet(output_path)
        print(f"ƒê√£ l∆∞u ML dataset t·∫°i: {output_path}")
        
        # In th·ªëng k√™
        print(f"T·ªïng s·ªë records trong ML dataset: {ml_df.count()}")
        print("Schema c·ªßa ML dataset:")
        ml_df.printSchema()
        
        # Show sample
        print("Sample data:")
        ml_df.show(5, truncate=False)
        
    except Exception as e:
        print(f"L·ªói khi l∆∞u ML dataset: {e}")

def save_pipeline_model(pipeline_model, model_path):
    """L∆∞u pipeline model ƒë·ªÉ s·ª≠ d·ª•ng sau"""
    try:
        pipeline_model.write().overwrite().save(model_path)
        print(f"ƒê√£ l∆∞u pipeline model t·∫°i: {model_path}")
    except Exception as e:
        print(f"L·ªói khi l∆∞u pipeline model: {e}")

def calculate_correlation_matrix(df, pipeline_model, save_path=None):
    """T√≠nh ma tr·∫≠n t∆∞∆°ng quan t·ª´ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c transform"""
    try:
        print("6. T√≠nh ma tr·∫≠n t∆∞∆°ng quan...")
        
        # Transform d·ªØ li·ªáu b·∫±ng pipeline ƒë√£ fit
        transformed_df = pipeline_model.transform(df)
        
        # L·∫•y c√°c c·ªôt numeric ƒë·ªÉ t√≠nh correlation
        numeric_cols = ["customer_care_calls", "customer_rating", "cost_of_product", 
                       "prior_purchases", "discount_offered", "weight_in_gms"]
        
        # Th√™m c√°c c·ªôt index t·ª´ categorical features
        categorical_index_cols = []
        categorical_cols = ["warehouse_block", "mode_of_shipment", "product_importance"]
        if "gender" in df.columns:
            categorical_cols.append("gender")
        
        for col_name in categorical_cols:
            if col_name in df.columns:
                categorical_index_cols.append(f"{col_name}_index")
        
        # Th√™m target variable n·∫øu c√≥
        target_cols = []
        if "reached_on_time" in df.columns:
            # T·∫°o numeric version c·ªßa target
            transformed_df = transformed_df.withColumn("reached_on_time_numeric", 
                                                     col("reached_on_time").cast("double"))
            target_cols.append("reached_on_time_numeric")
        
        # G·ªôp t·∫•t c·∫£ c·ªôt ƒë·ªÉ t√≠nh correlation
        correlation_cols = [col for col in numeric_cols if col in df.columns] + \
                          categorical_index_cols + target_cols
        
        print(f"C√°c c·ªôt ƒë·ªÉ t√≠nh correlation: {correlation_cols}")
        
        # Chuy·ªÉn sang Pandas ƒë·ªÉ t√≠nh correlation (v√¨ PySpark kh√¥ng c√≥ correlation matrix built-in)
        if len(correlation_cols) > 1:
            # Select v√† filter null values
            corr_df = transformed_df.select(correlation_cols).na.drop()
            
            # Convert to Pandas
            corr_pandas = corr_df.toPandas()
            
            # T√≠nh correlation matrix
            correlation_matrix = corr_pandas.corr()
            
            print("Ma tr·∫≠n t∆∞∆°ng quan:")
            print(correlation_matrix.round(3))
            
            # Visualize correlation matrix
            visualize_correlation_matrix(correlation_matrix, save_path)
            
            # L∆∞u correlation matrix th√†nh CSV
            if save_path:
                csv_path = f"{save_path}_correlation_matrix.csv"
                correlation_matrix.to_csv(csv_path)
                print(f"ƒê√£ l∆∞u correlation matrix t·∫°i: {csv_path}")
            
            return correlation_matrix
        else:
            print("Kh√¥ng ƒë·ªß c·ªôt numeric ƒë·ªÉ t√≠nh correlation matrix")
            return None
            
    except Exception as e:
        print(f"L·ªói khi t√≠nh correlation matrix: {e}")
        return None

def visualize_correlation_matrix(correlation_matrix, save_path=None):
    """V·∫Ω heatmap cho correlation matrix"""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # T·∫°o figure v·ªõi k√≠ch th∆∞·ªõc ph√π h·ª£p
        plt.figure(figsize=(12, 10))
        
        # T·∫°o heatmap
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle
        sns.heatmap(correlation_matrix, 
                    annot=True, 
                    cmap='coolwarm', 
                    center=0,
                    fmt='.2f',
                    mask=mask,
                    square=True,
                    cbar_kws={"shrink": .8})
        
        plt.title('Ma Tr·∫≠n T∆∞∆°ng Quan (Correlation Matrix)', fontsize=16, pad=20)
        plt.xlabel('Features', fontsize=12)
        plt.ylabel('Features', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # L∆∞u figure n·∫øu c√≥ ƒë∆∞·ªùng d·∫´n
        if save_path:
            plot_path = f"{save_path}_correlation_plot.png"
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ƒê√£ l∆∞u correlation plot t·∫°i: {plot_path}")
        
        # Hi·ªÉn th·ªã plot
        plt.show()
        
        # In c√°c correlation cao nh·∫•t
        print("\nüîç TOP CORRELATIONS:")
        print("=" * 50)
        
        # Flatten correlation matrix v√† sort
        corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = correlation_matrix.iloc[i, j]
                if not pd.isna(corr_val):
                    corr_pairs.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j], 
                        'correlation': corr_val
                    })
        
        # Sort theo absolute correlation
        corr_pairs.sort(key=lambda x: abs(x['correlation']), reverse=True)
        
        # In top 10 correlations
        print("Top 10 highest correlations:")
        for i, pair in enumerate(corr_pairs[:10]):
            print(f"{i+1:2d}. {pair['feature1']:<25} vs {pair['feature2']:<25}: {pair['correlation']:+.3f}")
            
    except Exception as e:
        print(f"L·ªói khi v·∫Ω correlation matrix: {e}")

def analyze_feature_importance_from_correlation(correlation_matrix, target_col="reached_on_time_numeric"):
    """Ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa features d·ª±a tr√™n correlation v·ªõi target"""
    try:
        if target_col in correlation_matrix.columns:
            # L·∫•y correlation v·ªõi target variable
            target_correlations = correlation_matrix[target_col].abs().sort_values(ascending=False)
            
            print(f"\nüìä FEATURE IMPORTANCE (based on correlation with {target_col}):")
            print("=" * 70)
            
            for i, (feature, corr) in enumerate(target_correlations.items()):
                if feature != target_col and not pd.isna(corr):
                    importance_level = "üî¥ High" if corr > 0.5 else "üü° Medium" if corr > 0.3 else "üü¢ Low"
                    print(f"{i+1:2d}. {feature:<30}: {corr:.3f} {importance_level}")
            
            return target_correlations
        else:
            print(f"Target column '{target_col}' not found in correlation matrix")
            return None
            
    except Exception as e:
        print(f"L·ªói khi ph√¢n t√≠ch feature importance: {e}")
        return None

def main():
    """H√†m ch√≠nh cho ML preprocessing"""
    spark = create_spark_session()
    
    try:
        # ƒê·ªçc d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c cleaned
        cleaned_data_path = FILE_PATHS["preprocessed_data"] + "/*.csv"
        print("=== B·∫ÆT ƒê·∫¶U ML PREPROCESSING ===")
        
        # 1. Load cleaned data
        print("1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ cleaned...")
        df = load_cleaned_data(spark, cleaned_data_path)
        if df is None:
            print("Kh√¥ng th·ªÉ ƒë·ªçc d·ªØ li·ªáu. H√£y ch·∫°y spark_preprocessing.py tr∆∞·ªõc.")
            return
        
        # 2. Chu·∫©n b·ªã ML pipeline
        print("2. T·∫°o ML preprocessing pipeline...")
        pipeline = prepare_ml_features(df)
        
        # 3. Transform data
        print("3. Transform d·ªØ li·ªáu cho ML...")
        ml_df, pipeline_model = create_ml_dataset(df, pipeline)
        
        # 4. L∆∞u ML dataset
        print("4. L∆∞u ML dataset...")
        ml_output_path = "ml_dataset"
        save_ml_dataset(ml_df, ml_output_path)
        
        # 5. L∆∞u pipeline model
        print("5. L∆∞u pipeline model...")
        model_path = "ml_pipeline_model"
        save_pipeline_model(pipeline_model, model_path)
        
        # 6. T√≠nh correlation matrix
        correlation_matrix = calculate_correlation_matrix(df, pipeline_model, "correlation_analysis")
        
        # 7. Ph√¢n t√≠ch feature importance
        if correlation_matrix is not None:
            feature_importance = analyze_feature_importance_from_correlation(correlation_matrix)
        
        print("=== HO√ÄN TH√ÄNH ML PREPROCESSING ===")
        
        return ml_df, pipeline_model, correlation_matrix
        
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
